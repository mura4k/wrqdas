{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install albumentations pyctcdecode pyewts botok huggingface_hub natsort"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "from zipfile import ZipFile\n",
        "import os\n",
        "\n",
        "# Download dataset\n",
        "dataset_path = snapshot_download(repo_id=\"BDRC/Karmapa8\", repo_type=\"dataset\", cache_dir=\"Datasets\")\n",
        "with ZipFile(f\"{dataset_path}/data.zip\", 'r') as zip:\n",
        "    zip.extractall(f\"{dataset_path}/Dataset\")\n",
        "dataset_path = os.path.join(dataset_path, \"Dataset\")\n",
        "\n",
        "# Download model\n",
        "model_path = snapshot_download(repo_id=\"BDRC/BigUCHAN_v1\", repo_type=\"model\", cache_dir=\"Models\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from BudaOCR.Modules import EasterNetwork, OCRTrainer, WylieEncoder\n",
        "from BudaOCR.Utils import create_dir, shuffle_data, build_data_paths, read_ctc_model_config\n",
        "import time\n",
        "\n",
        "model_config = f\"{model_path}/config.json\"\n",
        "ctc_config = read_ctc_model_config(model_config)\n",
        "\n",
        "label_encoder = WylieEncoder(ctc_config.charset)\n",
        "num_classes = label_encoder.num_classes()\n",
        "image_width = ctc_config.input_width\n",
        "image_height = ctc_config.input_height\n",
        "\n",
        "image_paths, label_paths = build_data_paths(dataset_path)\n",
        "image_paths, label_paths = shuffle_data(image_paths, label_paths)\n",
        "\n",
        "network = EasterNetwork(num_classes=num_classes, image_width=image_width, image_height=image_height, mean_pooling=True)\n",
        "network.fine_tune(f\"{model_path}/BigUCHAN_E_v1.pth\")\n",
        "\n",
        "output_dir = \"Output\"\n",
        "create_dir(output_dir)\n",
        "\n",
        "ocr_trainer = OCRTrainer(\n",
        "    network=network,\n",
        "    label_encoder=label_encoder,\n",
        "    workers=4,\n",
        "    image_width=image_width,\n",
        "    image_height=image_height,\n",
        "    batch_size=8,\n",
        "    output_dir=output_dir,\n",
        "    preload_labels=True\n",
        ")\n",
        "\n",
        "ocr_trainer.init(image_paths, label_paths)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "from glob import glob\n",
        "import time\n",
        "import torch\n",
        "\n",
        "def get_latest_checkpoint(output_dir: str):\n",
        "    ckpts = glob(os.path.join(output_dir, \"checkpoint_epoch_*.pth\"))\n",
        "    if not ckpts:\n",
        "        return None, 0\n",
        "    ckpts.sort(key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
        "    latest_ckpt = ckpts[-1]\n",
        "    epoch_num = int(latest_ckpt.split(\"_\")[-1].split(\".\")[0])\n",
        "    return latest_ckpt, epoch_num\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ —á–µ–∫–ø–æ–∏–Ω—Ç—ã –≤ Output\n",
        "latest_ckpt, start_epoch = get_latest_checkpoint(ocr_trainer.output_dir)\n",
        "\n",
        "if latest_ckpt:\n",
        "    print(f\"üîÅ Resuming from local Output checkpoint: {latest_ckpt} (epoch {start_epoch})\")\n",
        "    network.load_checkpoint(latest_ckpt)\n",
        "else:\n",
        "    # –ï—Å–ª–∏ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –Ω–µ—Ç, –ø—Ä–æ–≤–µ—Ä—è–µ–º Google Drive\n",
        "    drive_backup_dir = \"/content/drive/MyDrive/ocr_checkpoints\"\n",
        "    if os.path.exists(drive_backup_dir):\n",
        "        print(\"üì¶ Restoring checkpoint from Google Drive...\")\n",
        "        shutil.copytree(drive_backup_dir, ocr_trainer.output_dir, dirs_exist_ok=True)\n",
        "        latest_ckpt, start_epoch = get_latest_checkpoint(ocr_trainer.output_dir)\n",
        "        if latest_ckpt:\n",
        "            print(f\"üîÅ Resuming from Google Drive checkpoint: {latest_ckpt} (epoch {start_epoch})\")\n",
        "            network.load_checkpoint(latest_ckpt)\n",
        "        else:\n",
        "            print(\"üö® No checkpoint found after copying from Drive, starting fresh.\")\n",
        "            start_epoch = 0\n",
        "    else:\n",
        "        print(\"üìÇ No checkpoint found locally or in Google Drive, starting fresh.\")\n",
        "        start_epoch = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ú–æ–Ω—Ç–∏—Ä—É–µ–º Google Drive\n",
        "from google.colab import drive\n",
        "!fusermount -u /content/drive || true\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# –ü–∞–ø–∫–∞ –¥–ª—è —Ä–µ–∑–µ—Ä–≤–Ω—ã—Ö –∫–æ–ø–∏–π –≤ Drive\n",
        "drive_ckpt_dir = \"/content/drive/MyDrive/ocr_checkpoints\"\n",
        "os.makedirs(drive_ckpt_dir, exist_ok=True)\n",
        "\n",
        "# –û–±—É—á–µ–Ω–∏–µ –ø–æ —ç–ø–æ—Ö–∞–º —Å –∞–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∏ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤ –≤ Drive\n",
        "total_epochs = 64\n",
        "for epoch in range(start_epoch, total_epochs):\n",
        "    print(f\"[{time.ctime()}] üß™ Epoch {epoch+1}/{total_epochs}\")\n",
        "\n",
        "    # –û–±—É—á–∞–µ–º –æ–¥–Ω—É —ç–ø–æ—Ö—É\n",
        "    ocr_trainer.train(epochs=1, check_cer=False, export_onnx=False)\n",
        "\n",
        "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç –∫–∞–∂–¥—ã–µ 5 —ç–ø–æ—Ö\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_name = f\"checkpoint_epoch_{epoch+1}.pth\"\n",
        "        ckpt_path = os.path.join(ocr_trainer.output_dir, ckpt_name)\n",
        "        drive_ckpt_path = os.path.join(drive_ckpt_dir, ckpt_name)\n",
        "\n",
        "        torch.save(network.get_checkpoint(), ckpt_path)\n",
        "        assert os.path.isfile(ckpt_path), f\"‚ùå Failed to save checkpoint locally: {ckpt_path}\"\n",
        "\n",
        "        shutil.copy(ckpt_path, drive_ckpt_path)\n",
        "        print(f\"[{time.ctime()}] üíæ Checkpoint saved and copied to Drive: {drive_ckpt_path}\")\n",
        "\n",
        "        with open(os.path.join(ocr_trainer.output_dir, \"training_log.txt\"), \"a\") as log:\n",
        "            log.write(f\"[{time.ctime()}] Epoch {epoch+1}: checkpoint saved and synced to Drive.\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –í –∫–æ–Ω—Ü–µ —ç–∫—Å–ø–æ—Ä—Ç –º–æ–¥–µ–ª–∏ –≤ ONNX\n",
        "network.export_onnx(out_dir=ocr_trainer.output_dir, model_name=\"OCRModel\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "!fusermount -u /content/drive || true  # –±–µ–∑–æ–ø–∞—Å–Ω–æ —Ä–∞–∑–º–æ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å, –µ—Å–ª–∏ —É–∂–µ –µ—Å—Ç—å\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "drive_backup_dir = \"/content/drive/MyDrive/ocr_checkpoints\"\n",
        "\n",
        "if os.path.exists(drive_backup_dir):\n",
        "    print(\"üì¶ Restoring checkpoint from Google Drive...\")\n",
        "    shutil.copytree(drive_backup_dir, ocr_trainer.output_dir, dirs_exist_ok=True)\n",
        "else:\n",
        "    print(\"üìÇ No previous checkpoint found in Google Drive.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "total_epochs = 64\n",
        "drive_ckpt_dir = \"/content/drive/MyDrive/ocr_checkpoints\"\n",
        "os.makedirs(drive_ckpt_dir, exist_ok=True)\n",
        "\n",
        "for epoch in range(start_epoch, total_epochs):\n",
        "    print(f\"[{time.ctime()}] üß™ Epoch {epoch+1}/{total_epochs}\")\n",
        "    \n",
        "    # –æ–¥–Ω–∞ —ç–ø–æ—Ö–∞ –æ–±—É—á–µ–Ω–∏—è\n",
        "    ocr_trainer.train(epochs=1, check_cer=False, export_onnx=False)\n",
        "\n",
        "    # —Å–æ—Ö—Ä–∞–Ω—è–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç –∫–∞–∂–¥—ã–µ 5 —ç–ø–æ—Ö\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_name = f\"checkpoint_epoch_{epoch+1}.pth\"\n",
        "        ckpt_path = os.path.join(ocr_trainer.output_dir, ckpt_name)\n",
        "        drive_ckpt_path = os.path.join(drive_ckpt_dir, ckpt_name)\n",
        "\n",
        "        # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞ –¥–∏—Å–∫\n",
        "        torch.save(network.get_checkpoint(), ckpt_path)\n",
        "        assert os.path.isfile(ckpt_path), f\"‚ùå Failed to save checkpoint locally: {ckpt_path}\"\n",
        "\n",
        "        # –∫–æ–ø–∏—Ä—É–µ–º –≤ Google Drive\n",
        "        shutil.copy(ckpt_path, drive_ckpt_path)\n",
        "        print(f\"[{time.ctime()}] üíæ Checkpoint saved and copied to Drive: {drive_ckpt_path}\")\n",
        "\n",
        "        # –ª–æ–≥–∏—Ä—É–µ–º\n",
        "        with open(os.path.join(ocr_trainer.output_dir, \"training_log.txt\"), \"a\") as log:\n",
        "            log.write(f\"[{time.ctime()}] Epoch {epoch+1}: checkpoint saved and synced to Drive.\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –û—Ç–¥–µ–ª—å–Ω—ã–π —ç–∫—Å–ø–æ—Ä—Ç ONNX ‚Äî –±–µ–∑–æ–ø–∞—Å–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω–æ\n",
        "network.export_onnx(out_dir=ocr_trainer.output_dir, model_name=\"OCRModel\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cer_scores = ocr_trainer.evaluate()\n",
        "cer_values = list(cer_scores.values())\n",
        "\n",
        "with open(os.path.join(ocr_trainer.output_dir, \"cer_scores.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    for sample, value in cer_scores.items():\n",
        "        f.write(f\"{sample} - {value}\\\\n\")\n",
        "\n",
        "with open(os.path.join(ocr_trainer.output_dir, \"cer_summary.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(f\"Mean CER: {np.mean(cer_values)}\\\\n\")\n",
        "    f.write(f\"Max CER: {np.max(cer_values)}\\\\n\")\n",
        "    f.write(f\"Min CER: {np.min(cer_values)}\\\\n\")\n",
        "\n",
        "# Export ONNX\n",
        "network.export_onnx(out_dir=ocr_trainer.output_dir, model_name=\"OCRModel\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
